{
  "name": "VGML workshop @NIPS2016",
  "tagline": "",
  "body": "### Welcome the tentative NIPS workshop: Video Games and Machine Learning\r\nGood benchmarks are necessary for developing artificial intelligence. Recently, there has been a growing movement for the use of video games as machine learning benchmarks, and also an interest in the applications of machine learning from the video games community. While games have been used for AI research for a long time, only recently have we seen modern machine learning methods applied to video games.\r\n\r\nThis workshop focuses on complex games which provide interesting and hard challenges for machine learning. Going beyond the simple toy problems of the past and games which can easily be solved with search, we focus on games where learning is likely to be necessary to play well. This includes strategy games such as StarCraft, open-world games such as MineCraft,  first-person shooters such as Doom, as well as hard and unsolved 2D games such as Ms. Pac-Man and Montezuma's Revenge. While we see most of the challenges in game-playing, there are also interesting machine learning challenges in modeling and content generation.\r\n\r\n### Organization\r\nduration: 1 day  \r\ndate TBD  \r\nOrganisers:\r\n * Gabriel Synnaeve (gab@fb.com, Facebook AI Research)\r\n * Julian Togelius (julian.togelius@gmail.com, NYU)\r\n * Tom Schaul (schaul@gmail.com, Google DeepMind)\r\n * Nicolas Usunier (usunier@fb.com, Facebook AI Research)\r\n\r\n### Call for contributed posters\r\nTODO SMOOTH  \r\nThe machine learning community is more and more interested in games with complex dynamics or an adversarial environment. This is the setup of several video games, which are part of an existing research community (“Games AI”, “Artificial Intelligence and Interactive Digital Entertainment”, “Computational Intelligence and Games”). We propose to have a workshop focusing solely on games which are at the frontier of AI, because they require to solve problems to which we think ML could contribute. The search space in the game representation should be intractable through any form of (possibly Monte Carlo) tree search. In particular, these problems arise in the presence of complex environment dynamics, or partial observation, or extrinsic rewards reached only through some planning. This explicitly includes games like Doom, StarCraft, MineCraft, Ms Pac-Man, Montezuma's Revenge, some of the scenarios in GVGAI, MazeBase... This explicitly excludes that the sole focus of a paper should be on some of the classic reinforcement learning benchmarks (mountain car, cartpole), prisoner’s dilemma, Checkers, Chess. Go was at this frontier a year ago. AlphaGo is a recent exhibit of revisiting a “classic” game AI problem through the lens of supervised learning from human data and reinforcement learning for self-play, while building on the existing advances in Monte Carlo Tree Search. Most recent video games (and some quite old) are susceptible to fit these criteria, thus we intend to have speakers from the video games industry and researchers whose main domain of contribution is not machine learning (phrase this better).\r\n\r\nTODO FORMATING INSTRUCTIONS\r\n\r\n### Invited Speakers\r\nTODO COMPLETE\r\n * Santiago Ontanon\r\n * Katja Hofmann\r\n * Honglak Lee\r\n * Spyros Samothrakis\r\n * Alex Champandard",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}